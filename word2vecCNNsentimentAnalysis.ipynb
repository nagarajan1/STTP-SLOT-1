{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vecCNNsentimentAnalysis",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM1LU1sL0cf1bLxnjJ7AnSc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagarajan1/STTP-SLOT-1/blob/master/word2vecCNNsentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mzalR3YocmP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92f93d61-2a60-4e95-f041-880b7254c148"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from keras.layers.core import Dense, SpatialDropout1D\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.pooling import GlobalMaxPooling1D\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "import collections\n",
        "import nltk\n",
        "import numpy as np\n",
        "import codecs\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQoKsStNrRBx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "216f7f30-8f6d-4690-ea38-aea337fdf1cf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJGOiqHupvpg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from __future__ import absolute_import\n",
        "from __future__ import unicode_literals\n",
        "from time import gmtime, strftime\n",
        "from keras.callbacks import TensorBoard\n",
        "import os\n",
        "\n",
        "\n",
        "def make_tensorboard(set_dir_name='',\n",
        "                     embeddings_freq=0,\n",
        "                     embeddings_layer_names=None,\n",
        "                     embeddings_metadata=None):\n",
        "    tictoc = strftime(\"%a_%d_%b_%Y_%H_%M_%S\", gmtime())\n",
        "    directory_name = tictoc\n",
        "    log_dir = set_dir_name + '_' + directory_name\n",
        "    os.mkdir(log_dir)\n",
        "    tensorboard = TensorBoard(log_dir=log_dir,\n",
        "                              embeddings_freq=embeddings_freq,\n",
        "                              embeddings_layer_names=embeddings_layer_names,\n",
        "                              embeddings_metadata=embeddings_metadata)\n",
        "    return tensorboard, log_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jzjv3CppXzh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "738824aa-152c-45c9-ba63-f13692d73848"
      },
      "source": [
        "#from make_tensorboard import make_tensorboard"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-6b064a6edfc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmake_tensorboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_tensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'make_tensorboard'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuMwiksdqB6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "INPUT_FILE = \"drive/My Drive/training.txt\"\n",
        "# You have to get the model\n",
        "#    https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download\n",
        "WORD2VEC_MODEL = \"drive/My Drive/GoogleNews-vectors-negative300.bin.gz\"\n",
        "VOCAB_SIZE = 5000\n",
        "EMBED_SIZE = 300\n",
        "NUM_FILTERS = 256\n",
        "NUM_WORDS = 3\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub9YWQ7FqNA0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "48f65c49-1cbb-43b3-8e35-a3debba0b7f4"
      },
      "source": [
        "counter = collections.Counter()\n",
        "fin = codecs.open(INPUT_FILE, \"r\", encoding='utf-8')\n",
        "maxlen = 0\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "for line in fin:\n",
        "    _, sent = line.strip().split(\"\\t\")\n",
        "    words = [x.lower() for x in nltk.word_tokenize(sent)]\n",
        "    if len(words) > maxlen:\n",
        "        maxlen = len(words)\n",
        "    for word in words:\n",
        "        counter[word] += 1\n",
        "fin.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_kEB7tSqXjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2index = collections.defaultdict(int)\n",
        "for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\n",
        "    word2index[word[0]] = wid + 1\n",
        "vocab_sz = len(word2index) + 1\n",
        "index2word = {v: k for k, v in word2index.items()}\n",
        "\n",
        "xs, ys = [], []\n",
        "fin = codecs.open(INPUT_FILE, \"r\", encoding='utf-8')\n",
        "for line in fin:\n",
        "    label, sent = line.strip().split(\"\\t\")\n",
        "    ys.append(int(label))\n",
        "    words = [x.lower() for x in nltk.word_tokenize(sent)]\n",
        "    wids = [word2index[word] for word in words]\n",
        "    xs.append(wids)\n",
        "fin.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpKHvTW5qeoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = pad_sequences(xs, maxlen=maxlen)\n",
        "Y = np_utils.to_categorical(ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ2wfTkNqkY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f605ee03-5fef-4036-904f-cd7c5ac8c3ba"
      },
      "source": [
        "Xtrain, Xtest, Ytrain, Ytest = \\\n",
        "    train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4960, 42) (2126, 42) (4960, 2) (2126, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eycbZG-8qudj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f9e24e07-2c25-4c4b-fe0e-c9b719990ecd"
      },
      "source": [
        "# load word2vec model\n",
        "word2vec = KeyedVectors.load_word2vec_format(WORD2VEC_MODEL, binary=True)\n",
        "embedding_weights = np.zeros((vocab_sz, EMBED_SIZE))\n",
        "for word, index in word2index.items():\n",
        "    try:\n",
        "        embedding_weights[index, :] = word2vec[word]\n",
        "    except KeyError:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO-WQWfHq09V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "6992a0cd-afc7-4367-d17b-92405d7bbc11"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_sz, EMBED_SIZE, input_length=maxlen,\n",
        "                    weights=[embedding_weights],\n",
        "                    trainable=True))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS,\n",
        "                 activation=\"relu\"))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(2, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "tensorboard, log_dir = make_tensorboard(\n",
        "    set_dir_name='keras_finetune_word2vec_embeddings')\n",
        "\n",
        "history = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    callbacks=[tensorboard],\n",
        "                    validation_data=(Xtest, Ytest))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 4960 samples, validate on 2126 samples\n",
            "Epoch 1/10\n",
            "4960/4960 [==============================] - 10s 2ms/step - loss: 0.1253 - accuracy: 0.9532 - val_loss: 0.0229 - val_accuracy: 0.9934\n",
            "Epoch 2/10\n",
            "4960/4960 [==============================] - 10s 2ms/step - loss: 0.0113 - accuracy: 0.9972 - val_loss: 0.0155 - val_accuracy: 0.9958\n",
            "Epoch 3/10\n",
            "4960/4960 [==============================] - 10s 2ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.0140 - val_accuracy: 0.9939\n",
            "Epoch 4/10\n",
            "4960/4960 [==============================] - 10s 2ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.0141 - val_accuracy: 0.9962\n",
            "Epoch 5/10\n",
            "4960/4960 [==============================] - 10s 2ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.0146 - val_accuracy: 0.9929\n",
            "Epoch 6/10\n",
            "4960/4960 [==============================] - 10s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.0137 - val_accuracy: 0.9944\n",
            "Epoch 7/10\n",
            "4960/4960 [==============================] - 10s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0143 - val_accuracy: 0.9962\n",
            "Epoch 8/10\n",
            "4960/4960 [==============================] - 10s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0170 - val_accuracy: 0.9972\n",
            "Epoch 9/10\n",
            "4960/4960 [==============================] - 10s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0144 - val_accuracy: 0.9958\n",
            "Epoch 10/10\n",
            "4960/4960 [==============================] - 10s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0144 - val_accuracy: 0.9948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5qSmO3yq7v8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3f052d03-a5ac-41ea-df2e-947471b959c2"
      },
      "source": [
        "# evaluate model\n",
        "score = model.evaluate(Xtest, Ytest, verbose=1)\n",
        "print(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score[0], score[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2126/2126 [==============================] - 1s 370us/step\n",
            "Test score: 0.014, accuracy: 0.995\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}